{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import FFNet, tv_model\n",
    "from learning import Learn, Selector\n",
    "from dataset import SKDS, TVDS\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'D_in': 128, \n",
    "                'H': 512, \n",
    "                'D_out': 1, \n",
    "                'model_name': 'funnel'}\n",
    "\n",
    "ds_params = {'make': 'make_regression',\n",
    "             'sk_params': {'n_samples': 10000,\n",
    "                           'n_features': 128,\n",
    "                           'transform': transforms.Compose(\n",
    "                                               [transforms.Resize(224),\n",
    "                                                transforms.ToTensor()])}}\n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "crit_params = {'reduction': 'sum'}\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.1,.8)}\n",
    "sched_params = {'factor': .1,\n",
    "                'patience': 2}\n",
    "\n",
    "\n",
    "l = Learn(SKDS, FFNet, Selector, Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=MSELoss,\n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          adapt=False, load_model=False, load_embed=False, save_model=False,\n",
    "          batch_size=256, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'D_in': 784, \n",
    "                'H': 1024, \n",
    "                'D_out': 10, \n",
    "                'model_name': 'funnel'}\n",
    "\n",
    "ds_params={'embed': None,\n",
    "           'train_params': {'dataset': 'MNIST',\n",
    "                            'tv_params':{'root': './data/',\n",
    "                                         'train': True,\n",
    "                                         'download': True,\n",
    "                                         'transform': transforms.Compose(\n",
    "                                                           [transforms.Resize(224),\n",
    "                                                            transforms.ToTensor()]),\n",
    "                                         'target_transform': None}\n",
    "                            \n",
    "            'test_params': {'dataset': 'MNIST',\n",
    "                            'tv_params':{'root': './data/',\n",
    "                                         'train': False,\n",
    "                                         'download': True,\n",
    "                                         'transform': transforms.Compose(\n",
    "                                                           [transforms.Resize(224),\n",
    "                                                            transforms.ToTensor()]),\n",
    "                                         'target_transform': None}\n",
    "        \n",
    "\n",
    "opt_params = {'lr': 0.01}\n",
    "crit_params = {'reduction': 'sum'}\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7,.15)}\n",
    "sched_params = {'factor': .1,\n",
    "                'patience': 2}\n",
    "\n",
    "l = Learn([TVDS,TVDS], FFNet, Selector, Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=MSELoss, \n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          adapt=False, load_model=False, load_embed=False, save_model=False,\n",
    "          batch_size=256, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([16, 1])) that is different to the input size (torch.Size([16, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Float\nException raised from compute_types at /opt/conda/conda-bld/pytorch_1595629403081/work/aten/src/ATen/native/TensorIterator.cpp:183 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x4d (0x7fa4646e177d in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: at::TensorIterator::compute_types(at::TensorIteratorConfig const&) + 0x259 (0x7fa443b66ca9 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #2: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x6b (0x7fa443b6a44b in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #3: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7fa443b6aabd in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #4: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x18a (0x7fa4439cf71a in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: <unknown function> + 0xd1d610 (0x7fa411c92610 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x90 (0x7fa4439cc140 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0xd1d6b0 (0x7fa411c926b0 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #8: <unknown function> + 0xd3f936 (0x7fa411cb4936 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #9: at::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x119 (0x7fa443e8eda9 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0x2b5e8c9 (0x7fa445ae78c9 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0x7f60d6 (0x7fa44377f0d6 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: at::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x119 (0x7fa443e8eda9 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1af (0x7fa445a2352f in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: <unknown function> + 0x30d1017 (0x7fa44605a017 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7fa446055860 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7fa446056401 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7fa44604e579 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7fa46507199a in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #19: <unknown function> + 0xc819d (0x7fa4812c419d in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6)\nframe #20: <unknown function> + 0x9609 (0x7fa483b45609 in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #21: clone + 0x43 (0x7fa483a6c293 in /lib/x86_64-linux-gnu/libc.so.6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-becae0c46c06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0mopt_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msched_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msched_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m           \u001b[0madapt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_embed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m           batch_size=16, epochs=10)\n\u001b[0m",
      "\u001b[0;32m~/cosmosis/learning.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, Datasets, Model, Sampler, Optimizer, Scheduler, Criterion, ds_params, model_params, sample_params, opt_params, sched_params, crit_params, adapt, load_model, load_embed, save_model, batch_size, epochs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle_train_val_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cosmosis/learning.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0me_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mb_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mflag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                     \u001b[0mb_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Double but expected Float\nException raised from compute_types at /opt/conda/conda-bld/pytorch_1595629403081/work/aten/src/ATen/native/TensorIterator.cpp:183 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x4d (0x7fa4646e177d in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: at::TensorIterator::compute_types(at::TensorIteratorConfig const&) + 0x259 (0x7fa443b66ca9 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #2: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x6b (0x7fa443b6a44b in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #3: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7fa443b6aabd in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #4: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x18a (0x7fa4439cf71a in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: <unknown function> + 0xd1d610 (0x7fa411c92610 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x90 (0x7fa4439cc140 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0xd1d6b0 (0x7fa411c926b0 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #8: <unknown function> + 0xd3f936 (0x7fa411cb4936 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #9: at::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x119 (0x7fa443e8eda9 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0x2b5e8c9 (0x7fa445ae78c9 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0x7f60d6 (0x7fa44377f0d6 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: at::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x119 (0x7fa443e8eda9 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1af (0x7fa445a2352f in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: <unknown function> + 0x30d1017 (0x7fa44605a017 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7fa446055860 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7fa446056401 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7fa44604e579 in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7fa46507199a in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #19: <unknown function> + 0xc819d (0x7fa4812c419d in /home/fltr/miniconda3/envs/cosmosis/lib/python3.7/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6)\nframe #20: <unknown function> + 0x9609 (0x7fa483b45609 in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #21: clone + 0x43 (0x7fa483a6c293 in /lib/x86_64-linux-gnu/libc.so.6)\n"
     ]
    }
   ],
   "source": [
    "model_params = {'model_name': 'resnet18',\n",
    "                'in_channels': 1,\n",
    "                'tv_params': {'num_classes': 10}}\n",
    "\n",
    "ds_params={'embed': None,\n",
    "           'ds_params': {'dataset': 'MNIST',\n",
    "                         'tv_params':{'root': './data/',\n",
    "                                      'train': True,\n",
    "                                      'download': True,\n",
    "                                      'transform': transforms.Compose(\n",
    "                                                        [transforms.Resize(224),\n",
    "                                                         transforms.ToTensor()]),\n",
    "                                      'target_transform': None}}}\n",
    "           \n",
    "opt_params = {'lr': 0.01}\n",
    "crit_params = {'reduction': 'sum'}\n",
    "sample_params = {'set_seed': 88,\n",
    "                 'splits': (.7,.15)}\n",
    "sched_params = {'factor': .1,\n",
    "                'patience': 10}\n",
    "\n",
    "l = Learn([TVDS], tv_model, Selector, Optimizer=Adam, Scheduler=ReduceLROnPlateau, Criterion=MSELoss, \n",
    "          model_params=model_params, ds_params=ds_params, sample_params=sample_params,\n",
    "          opt_params=opt_params, sched_params=sched_params, crit_params=crit_params,\n",
    "          adapt=False, load_model=False, load_embed=False, save_model=False,\n",
    "          batch_size=16, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_params={'dataset': 'MNIST',\n",
    "           'embed': None,\n",
    "           'tv_params': {'root': './data/',\n",
    "                         'train': True,\n",
    "                         'download': True,\n",
    "                         'transform': transforms.Compose(\n",
    "                                           [transforms.ToTensor()]),\n",
    "                         'target_transform': transforms.Compose(\n",
    "                                           [transforms.ToTensor()])}}\n",
    "           \n",
    "tvds = TVDS(**ds_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvds[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvds[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
